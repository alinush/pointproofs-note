\section{Preliminaries}

\parhead{Notation.}
Let $\Gr_1, \Gr_2, \Gr_T$ denote groups of prime order $p$.
Let $[N] = \{1,2,\dots, N\}$.
Let $\vect{m}=[m_1, m_2, \dots, m_N]$ be a vector of elements, indexed from 1 to $N$.
Let $\vect{m}^\top$ denote its transpose: a column vector whose $i$th entry is $m_i$.
Let $\diag(\vect{m})$ denote the $N\times N$ diagonal matrix whose entry at position $(i,i)$ is $m_i$ and all other entries are 0.
Also, $\diag(\vect{m})=\diag(\vect{m}^\top)$.
Let $\vect{m}[S] = (m_i)_{i \in S}$ be a \textit{subvector} of $\vect{m}$ consisting only of the positions $i\in S$.

\subsection{Discrete Fourier Transform (DFT) on group elements}
\label{s:dft}

Let $\omega_N$ denote a primitive $N$th root of unity in the finite field $\Zp$.
The \textit{Discrete Fourier Transform (DFT)} of a vector $\vect{x} = [x_1,\dots,x_N]^\top \in \Gr_1^N$ is defined as:
\begin{align}
    \dft(\vect{x}) &= \vect{\hat{x}} = [\hat{x}_1, \dots, \hat{x}_N]^\top\in \Gr_1^N,\ \text{where}\ \hat{x}_i= \prod_{j\in [N]} (x_j)^{(\omega_N)^{(i-1)(j-1)}},\forall i\in[N]
\end{align}
Importantly, the DFT can be computed in $O(N\log{N})$ time~\cite{CLRS09}.
Furthermore, it is invertible.
As we discuss next, one can define $\invdft(\cdot)$ such that $\invdft(\dft(\vect{x})) = \dft(\invdft(\vect{x})) = \vect{x}$.
%where:
%\begin{align}
%
%\end{align}

\subsubsection{The DFT matrix}
\label{s:dft:matrix}
%Suppose $\vect{x}$ is a size-$N$ row vector.
%Then,
$\dft(\vect{x})$ can also be computed as a matrix product as follows:
\begin{align}
    \dft(\vect{x}) = \matr{W}_N \vect{x}
\end{align}
Here, $\matr{W}_N$ is known as the \textit{DFT matrix}:
\begin{align}
    \matr{W}_N&=
    \begin{bmatrix}
        1&1&1&1&\cdots &1\\1&(\omega_N)^1&(\omega_N)^{2}&(\omega_N)^{3}&\cdots &(\omega_N)^{N-1}\\
        1&(\omega_N)^{2}&(\omega_N)^{4}&(\omega_N)^{6}&\cdots &(\omega_N)^{2(N-1)}\\
        1&(\omega_N)^{3}&(\omega_N)^{6}&(\omega_N)^{9}&\cdots &(\omega_N)^{3(N-1)}\\
        \vdots &\vdots &\vdots &\vdots &\ddots &\vdots \\
        1&(\omega_N)^{N-1}&(\omega_N)^{2(N-1)}&(\omega_N)^{3(N-1)}&\cdots &(\omega_N)^{(N-1)(N-1)}
    \end{bmatrix}
\end{align}
This makes it very easy to define the \textit{inverse DFT} of a size-$N$ column vector $\vect{y}$ as:
\begin{align}
    \invdft(\vect{y}) &= (\matr{W}_N)^{-1} \vect{y}\\
        &= \frac{1}{N} \matr{W}_N \vect{y}
\end{align}
Note that the inverse of the DFT matrix $\matr{W}_N$ is simply $\frac{1}{N} \matr{W}_N$.
Thus, an inverse DFT can be reduced to computing a normal DFT and scaling it by $1/N$.

%\parhead{An example.}
%The DFT matrix for $N=4$ is:
%\begin{align}
%\matr{W}_4=\begin{bmatrix}
%    1 & 1     & 1     & 1 \\\\\
%    1 & (\omega)^1  & (\omega)^2  & (\omega)^3 \\\\\
%    1 & (\omega^2)^1 & (\omega^2)^2 & (\omega^2)^3 \\\\\
%    1 & (\omega^3)^1 & (\omega^3)^2 & (\omega^3)^3
%\end{bmatrix}
%\end{align}
%Here, $\omega$ is a primitive 4th root of unity.

\subsection{Circulant matrices}
\label{s:circulant}

A \textit{circulant matrix} of size $N\times N$ is a matrix of the following form:
\begin{align}
    \matr{C}_N&=\begin{bmatrix}
        c_{0}   & c_{N-1} & c_{N-2} & \cdots & \cdots  & c_{1}\\
        c_{1}   & c_{0}   & c_{N-1} & \ddots &         & \vdots\\
        c_{2}   & c_{1}   & \ddots  & \ddots & \ddots  & \vdots\\
        \vdots  & \ddots  & \ddots  & \ddots & c_{N-1} & c_{N-2}\\
        \vdots  &         & \ddots  & c_{1}  & c_{0}   & c_{N-1}\\
        c_{N-1} & \cdots  & \cdots  & c_{2}  & c_{1}   & c_{0}
    \end{bmatrix}
\end{align}

In other words, as you go from left to right, each column is set to the previous column but ``rotated down'' by one.
(Looked at differently, each row is set to the row above it ``rotated to the right'' by one.)

\parhead{Vector representation.}
Note that $\matr{C}_N$ is uniquely determined by its first column, which we denote by $\vect{c}_N = [c_0, c_1, c_2, \dots c_{N-1}]^\top$.

\parhead{An example.}
The following matrix is a circulant matrix:
\begin{align}
    \begin{bmatrix}
        \textbf{7}    & \g{3}        & \myyellow{8} & \underline{1}\\
        \g{3}         & \textbf{7}   & \g{3}        & \myyellow{8}\\
        \myyellow{8}  & \g{3}        & \textbf{7}   & \g{3}\\
        \underline{1} & \myyellow{8} & \g{3}        & \textbf{7}
    \end{bmatrix}
\end{align}

%\subsubsection{Diagonalization using the DFT matrix}
\subsubsection{Multiplying a circulant matrix by a vector}
\label{s:circulant:diag-dft}
\label{s:circulant:multiply-vec}
It is well-known that one can \textit{diagonalize}~\cite{Diag20} any circulant matrix $\matr{C}_N$ with vector representation $\vect{c}_N$ as:
\begin{align}
    \matr{C}_N
    %&= (\matr{W}_N)^{-1} \diag(\matr{W}_N \vect{c}_N) \matr{W}_N\\
     &= (\matr{W}_N)^{-1} \diag(\dft(\vect{c}_N)) \matr{W}_N
\end{align}
Let $m=[m_1,\dots,m_N]^\top$ be a vector.
Let $\vect{x}\circ\vect{y} = [x_1 y_1, x_2 y_2,\dots, x_N y_N]^\top$ denote the Hadamard product of two column vectors.
It is also well-known that the above diagonalization can be leveraged to efficiently compute matrix-vector products $\matr{C}_N\vect{m}$ as:
\begin{align}
    \matr{C}_N \vect{m}
        &= ((\matr{W}_N)^{-1} \diag(\dft(\vect{c}_N)) \matr{W}_N)\vect{m}\\
        &= (\matr{W}_N)^{-1} \diag(\dft(\vect{c}_N)) (\matr{W}_N\vect{m})\\
        &= ((\matr{W}_N)^{-1} \diag(\dft(\vect{c}_N))) \dft(\vect{m})\\
        &= (\matr{W}_N)^{-1} (\dft(\vect{c}_N)\circ \dft(\vect{m}))\\
        &= \invdft(\dft(\vect{c}_N)\circ \dft(\vect{m}))
\end{align}
In other words, such products by circulant matrices reduce to computing two DFTs and one inverse DFT.
Overall, this takes $O(N\log{N})$ time.
\\

\noindent\textit{Observation:} The DFT on $\vect{m}$ is a DFT on field elements, rather than group elements:
\begin{align}
    \dft(\vect{m}) &= \vect{\hat{m}} = [\hat{m}_1, \dots, \hat{m}_N]^\top\in \Zp,\ \text{where}\ \hat{m}_i= \prod_{j\in [N]} m_j (\omega_N)^{(i-1)(j-1)},\forall i\in[N]
\end{align}

\subsection{Toeplitz matrices}
\label{s:toeplitz}

A \textit{Toeplitz matrix} or \textit{diagonal-constant matrix} of size $N\times N$ is a matrix of the following form:
\begin{align}
\matr{A}_N&=\begin{bmatrix}
    a_{0}   & a_{-1} & a_{-2} & \cdots & \cdots & a_{-(N-1)}\\
    a_{1}   & a_{0}  & a_{-1} & \ddots &        & \vdots\\
    a_{2}   & a_{1}  & \ddots & \ddots & \ddots & \vdots\\
    \vdots  & \ddots & \ddots & \ddots & a_{-1} & a_{-2}\\
    \vdots  &        & \ddots & a_{1}  & a_{0}  & a_{-1}\\
    a_{N-1} & \cdots & \cdots & a_{2}  & a_{1}  & a_{0}
\end{bmatrix}
\end{align}
In other words, all of $\matr{A}_N$'s descending diagonals (from left to right) have the same entries.
To simplify notation, we are slightly abusing notation here and using negative indices for the entries of the matrix.
Note that a circulant matrix (see \cref{s:circulant}) is a special kind of Toeplitz matrix with $a_{-i} = a_{N-i},\forall i\in [N - 1]$.

\parhead{An example.}
The following matrix is a Toeplitz matrix:
\begin{align}
\begin{bmatrix}
    \textbf{7}    & \N{11}       & \myblue{5} & \textit{6}\\
    \g{3}         & \textbf{7}   & \N{11}     & \myblue{5}\\
    \myyellow{8}  & \g{3}        & \textbf{7} & \N{11}\\
    \underline{1} & \myyellow{8} & \g{3}      & \textbf{7}
\end{bmatrix}
\end{align}

\subsubsection{Multiplying a Toeplitz matrix by a vector}
\label{s:toeplitz:multiply-vec}
%Computing $\vect{y} = \matr{A}_N \vect{m}$ for any vector $\vect{m} = [m_1,\dots, m_N]^\top$ can be done in $O(N\log{N})$ time, if $\matr{A}_N$ is a Toeplitz matrix.
%The idea is to embed $\matr{A}_N$ inside a slightly bigger circulant matrix (see \cref{s:circulant}) in such a way that multiplying this circulant matrix by an extension $\vect{m'}$ of $\vect{m}$ can be used to compute $\matr{A}_N \cdot \vect{m}$.

%Specifically,
Suppose we want to multiply a Toeplitz matrix $\matr{A}_N$ by a size-$N$ colum vector $\vect{m}$.
Note that we can turn any such $\matr{A}_N$ into a circulant matrix of size $2N$ by finding another matrix $\matr{B}_N$ so that the following matrix is circulant:
\begin{align}
    \matr{C}_{2N} &= \begin{bmatrix}
        \matr{A}_N & \matr{B}_N\\
        \matr{B}_N & \matr{A}_N\\
    \end{bmatrix}
\end{align}
For now, assume we can find such a $\matr{B}_N$.
Then, let $\vect{m'} = \begin{bmatrix}\vect{m}\\\vect{0}_N\end{bmatrix}$ be an extension of the column vector $\vect{m}$ with $N$ extra zeros appended to it, which are denoted by the size-$N$ column vector $\vect{0}_N$.
Finally, note that:
\begin{align}
     \label{eq:toeplitz-in-circ}
     \matr{C}_{2N} \vect{m'}
     &= \begin{bmatrix}
         \matr{A}_N & \matr{B}_N\\
         \matr{B}_N & \matr{A}_N\\
     \end{bmatrix} \begin{bmatrix}
     \vect{m}\\
     \vect{0}_N
     \end{bmatrix}
     = \begin{bmatrix}
         \matr{A}_N \vect{m}\\
         \matr{B}_N \vect{m}\\
     \end{bmatrix}
\end{align}
In other words, we can compute $\matr{A}_N \vect{m}$ by computing $\matr{C}_{2N}\vect{m'}$, which can be done in $O(N\log{N})$ time (see \cref{s:circulant:multiply-vec}).

\parhead{How to find the matrix $\matr{B}_N$?}
First, let us take a small example $\matr{A}_4$:
\begin{align}
    \matr{A}_4 = \begin{bmatrix}
        \myperi{a_0}    & \N{a_{-1}}   & \myblue{a_{-2}} & \mypink{a_{-3}}\\
        \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}     & \myblue{a_{-2}}\\
        \myyellow{a_2}  & \g{a_1}      & \myperi{a_0} & \N{a_{-1}}\\
        \mygblue{a_3} & \myyellow{a_2} & \g{a_1}    & \myperi{a_0}
    \end{bmatrix}
\end{align}
Let us start embedding it in a circulant matrix $\matr{C}_8$:
\begin{align}
    \matr{C}_8 &= \begin{bmatrix}
        \matr{A}_4 & \matr{B}_4\\
        \matr{B}_4 & \matr{A}_4\\
    \end{bmatrix} = \begin{bmatrix}
        \myperi{a_0}    & \N{a_{-1}}     & \myblue{a_{-2}} & \mypink{a_{-3}}
        & ? & ? & ? & ?\\
        \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}      & \myblue{a_{-2}}
        & ? & ? & ? & ?\\
        \myyellow{a_2}  & \g{a_1}        & \myperi{a_0}    & \N{a_{-1}}
        & ? & ? & ? & ?\\
        \mygblue{a_3}   & \myyellow{a_2} & \g{a_1}         & \myperi{a_0}
        & ? & ? & ? & ?\\
        ? & ? & ? & ? &
        \myperi{a_0}    & \N{a_{-1}}     & \myblue{a_{-2}} & \mypink{a_{-3}}\\
        ? & ? & ? & ? &
        \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}      & \myblue{a_{-2}}\\
        ? & ? & ? & ? &
        \myyellow{a_2}  & \g{a_1}        & \myperi{a_0}    & \N{a_{-1}}\\
        ? & ? & ? & ? &
        \mygblue{a_3}   & \myyellow{a_2} & \g{a_1}         & \myperi{a_0}
    \end{bmatrix}
\end{align}
The remaining task is to fill in the gaps that define $\matr{B}_4$ while ensuring $\matr{C}_8$ remains circulant.
In this case, we can easily fill in some of the gaps:
\begin{align}
    \matr{C}_8 &= \begin{bmatrix}
        \matr{A}_4 & \matr{B}_4\\
        \matr{B}_4 & \matr{A}_4\\
    \end{bmatrix} = \begin{bmatrix}
        \begin{array}{c c c c |c c c c}
        \myperi{a_0}    & \N{a_{-1}}     & \myblue{a_{-2}} & \mypink{a_{-3}}
        & ? & ? & ? & ?\\
        \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}      & \myblue{a_{-2}}
        & \mypink{a_{-3}} & ? & ? & ?\\
        \myyellow{a_2}  & \g{a_1}        & \myperi{a_0}    & \N{a_{-1}}
        & \myblue{a_{-2}} & \mypink{a_{-3}} & ? & ?\\
        \mygblue{a_3}   & \myyellow{a_2} & \g{a_1}         & \myperi{a_0}
        & \N{a_{-1}} & \myblue{a_{-2}} & \mypink{a_{-3}} & ?\\
        % Second half starts here!
        \hline
        ? &  \mygblue{a_3} & \myyellow{a_2} & \g{a_1} &
        \myperi{a_0}    & \N{a_{-1}}     & \myblue{a_{-2}} & \mypink{a_{-3}}\\
        ? & ? & \mygblue{a_3} & \myyellow{a_2} &
        \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}      & \myblue{a_{-2}}\\
        ? & ? & ? & \mygblue{a_3} &
        \myyellow{a_2}  & \g{a_1}        & \myperi{a_0}    & \N{a_{-1}}\\
        ? & ? & ? & ? &
        \mygblue{a_3}   & \myyellow{a_2} & \g{a_1}         & \myperi{a_0}
        \end{array}
    \end{bmatrix}
\end{align}
Note that the only thing missing is the descending diagonal of $\textbf{B}_4$, since we have defined all of its other entries above!
We can set its diagonal to be anything, say, $\myperi{a_0}$.
\begin{align}
    \matr{C}_8 &= \begin{bmatrix}
        \matr{A}_4 & \matr{B}_4\\
        \matr{B}_4 & \matr{A}_4\\
    \end{bmatrix} = \begin{bmatrix}
        \begin{array}{c c c c |c c c c}
            \myperi{a_0}    & \N{a_{-1}}     & \myblue{a_{-2}} & \mypink{a_{-3}}
            & \myperi{a_0} & \mygblue{a_3} & \myyellow{a_2} & \g{a_1}\\
            \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}      & \myblue{a_{-2}}
            & \mypink{a_{-3}} & \myperi{a_0} & \mygblue{a_3} & \myyellow{a_2}\\
            \myyellow{a_2}  & \g{a_1}        & \myperi{a_0}    & \N{a_{-1}}
            & \myblue{a_{-2}} & \mypink{a_{-3}} & \myperi{a_0} & \mygblue{a_3}\\
            \mygblue{a_3}   & \myyellow{a_2} & \g{a_1}         & \myperi{a_0}
            & \N{a_{-1}} & \myblue{a_{-2}} & \mypink{a_{-3}} & \myperi{a_0}\\
            % Second half starts here!
            \hline
            \myperi{a_0} &  \mygblue{a_3} & \myyellow{a_2} & \g{a_1} &
            \myperi{a_0}    & \N{a_{-1}}     & \myblue{a_{-2}} & \mypink{a_{-3}}\\
            \mypink{a_{-3}} & \myperi{a_0} & \mygblue{a_3} & \myyellow{a_2} &
            \g{a_1}         & \myperi{a_0}   & \N{a_{-1}}      & \myblue{a_{-2}}\\
            \myblue{a_{-2}} & \mypink{a_{-3}} & \myperi{a_0} & \mygblue{a_3} &
            \myyellow{a_2}  & \g{a_1}        & \myperi{a_0}    & \N{a_{-1}}\\
            \N{a_{-1}} & \myblue{a_{-2}} & \mypink{a_{-3}} & \myperi{a_0} &
            \mygblue{a_3}   & \myyellow{a_2} & \g{a_1}         & \myperi{a_0}
        \end{array}
    \end{bmatrix}
\end{align}

We can now generalize to any $N$!
Let $[a_0, a_1,\dots, a_{N-1}]^\top$ be the first column of $\matr{A}_N$, and let $[a_0, a_{-1}, a_{-2},\dots$, $a_{-(N-1)}]$ be the first row of $\matr{A}_N$, which together fully define the matrix $\matr{A}_N$.
We can let $\matr{B}_N$ be a Toeplitz matrix whose first column is:
\begin{align}
    [a_0,a_{-(N-1)},a_{-(N-2)},\dots,a_{-1}]^\top
\end{align}
and whose first row is:
\begin{align}
    [a_0,a_{N-1},a_{N-2},\dots,a_{1}]^\top
\end{align}
This ensures that $\matr{C}_{2N}$ is circulant, which allows us to multiply it by $\vect{m'}$ in $O(N\log{N})$ time (see \cref{s:circulant:multiply-vec}) and obtain $\matr{A}_N \vect{m}$ as per \cref{eq:toeplitz-in-circ}.
Note that the vector representation of $\matr{C}_{2N}$ is:
\begin{align}
    \vect{c}_{2N} = [a_0, a_1, \dots, a_{N-1}, a_0,a_{-(N-1)},a_{-(N-2)},\dots,a_{-1}]^\top
\end{align}

\subsection{Pointproofs}
\label{s:pointproofs}

Gorbunov et al.~\cite{GRWZ20} enhance the VC by Libert and Yung~\cite{LY10} with the ability to aggregate multiple proofs into a subvector proof.
Additionally, they also enable aggregation of subvector proofs across different vector commitments, which they show is useful for stateless smart contract validation in cryptocurrencies.

\parhead{Public parameters.}
Let $e : \Gr_1 \times \Gr_2\rightarrow \Gr_T$~\cite{GPS08} be a Type III pairing (i.e., $\Gr_1\ne \Gr_2$ and there is no efficiently computable homomorphisms between $\Gr_1$ and $\Gr_2$).
Let $g_1,g_2,g_T$ be generators of $\Gr_1, \Gr_2$ and $\Gr_T$ respectively, which everybody knows.
The $O(N)$-sized \textit{proving key} used to commit to a vector is:
\begin{align}
    \prk = (g_1^{\alpha},\dots, g_1^{\alpha^N}; g_1^{\alpha^{N+2}},\dots, g_1^{\alpha^{2N}})
\end{align}
The $O(N)$-sized \textit{verification key} used to verify proofs is:
\begin{align}
    \vrk = (g_2^{\alpha},\dots,g_2^{\alpha^N}; g_T^{\alpha^{N+1}})
\end{align}
Note that $g_1^{\alpha^{N+1}}$ is ``missing'' from the proving key, which is essential for security.
%They only support updating commitments, but proofs could be made updatable at the cost of linear-sized update keys.

\parhead{Commitment.}
A commitment $C$ to a vector $\vect{m}=[m_1,\dots,m_N]$ is:
\begin{align}
    C = \prod_{i\in[N]} \left(g_1^{\alpha^i}\right)^{m_i}
      = g_1^{\sum_{i\in [N]} m_i\alpha^i}
\end{align}
The commitment can be computed with $O(N)$ exponentiations.

The $i$th \textit{update key} is $\upk_i=g_1^{\alpha^i}$.
If any vector element $m_j$ changes to $m_j + \delta$, the commitment can be updated in $O(1)$ time as $c' = c \cdot (\upk_j)^{\delta} = c\cdot (g_1^{\alpha^j})^{\delta}$.

\parhead{Proofs for a $m_i$.}
A proof for $m_i$ is obtained by re-committing to $v$ so that $m_i$ ``lands'' at position $N+1$ (i.e., has coefficient $\alpha^{N+1}$) rather than ``landing'' at position $i$ (i.e., has coefficient $\alpha^i$).
Furthermore, this commitment will \textbf{not} contain $m_i$: it cannot, since that would require knowledge of $g_1^{\alpha^{N+1}}$, which is not part of the proving key \prk.
To get position $i$ to $N+1$, we must ``shift'' it (and every other position) by $(N + 1) - i$.
Thus, the proof is:
\begin{align}
    \label{eq:indiv-proof}
    \pi_i &= g_1^{\sum_{j\in[N]\setminus\{i\}} m_j \alpha^{j + (N+1) - i}}\\
    %&= g_1^{\sum_{j\in[N]\setminus\{i\}} m_j \alpha^{j} \alpha^{(N+1) - i}}\\
    &= \left(g_1^{\sum_{j\in[N]\setminus\{i\}} m_j \alpha^{j}}\right)^{\alpha^{(N+1) - i}}\\
    &= \left(\frac{g_1^{\sum_{j\in[N]} m_j \alpha^{j}}}{g_1^{m_i \alpha^i}}\right)^{\alpha^{(N+1) - i}}\\
    &= (C / g_1^{m_i \alpha^i})^{\alpha^{(N+1) - i}}
\end{align}
The proof is constant-sized and can be computed with $O(N)$ exponentiations.
It can be verified in $O(1)$ time using $g_2^{\alpha^{(N+1) - i}}$ from \vrk:
\begin{align}
    e(C, g_2^{\alpha^{(N+1)-i}}) \stackrel{?}{=} e(\pi_i, g_2) \cdot g_T^{\alpha^{N+1} m_i}
\end{align}

\parhead{Other features.}
Updating proofs is not discussed but can be done in $O(1)$ time, if the $i$th update key $\upk_i$ is tweaked to be $O(N)$-sized rather than constant-sized.
In addition to computing proofs $\pi_i$ for a single vector element $m_i$, Pointproofs additionally supports computing constant-sized proofs for a \textit{subvector} $\vect{m}[S]$.
Importantly, Pointproofs supports aggregating many such $\pi_i, i\in S$ into a subvector proof $\pi_S$ $\vect{m}[S]$, as well as \textit{cross-aggregating} multiple subvector proofs $\hat{\pi}_j$ each for a different subvector $\vect{m}_j[S_j]$ and each with respect to a different commitment $C_j$, into a single proof $\pi$.
We refer the reader to the original paper for details on subvector proofs and (cross)aggregation~\cite{GRWZ20}.

\parhead{Precomputing all proofs.}
Precomputing all proofs efficiently in Pointproofs is not discussed.
Naively, it can be done in $O(N^2)$ time by computing each proof $\pi_i$ in $O(N)$ time.
In \cref{s:pointproofs:precompute-all-proofs}, we show how this can be done in $O(N\log{N})$ time using a Toeplitz matrix multiplication.
